Please Enter your team's full names and your answers to the questions marked by QS questions here!

Encho Genchev
Fabian Ochoa

Q1.1: 
ReflexAgent first checks all of the legal moves pacman can make and then determines
the best action based on an evaluation function. The original evaluation function did not update 
the "score" for each action, resulting in random moves. The updated evaluation function takes into
account pacman's proximity to food, giving actions closer to food a higher score. The updated 
evaulation function also gives a huge penalty of the action places pacman next to a ghost since that
could potentially mean game over. With these updates, pacman beats the single ghost everytime and
sometimes even beats the two ghosts.


Q1.2:
d = distance to food 
g = number ghosts pacman is next to
s = number of turns pacman stops

score = score + 10/d - 10s - 1000g 

This makes sense because the function returns higher values when pacman is moving and getting
closer to food, which would get higher scores. It also returns lower values when pacman stops or 
encounters a ghost, which could end the game or make it go on forever.


Q2.1: 
MinimaxAgent runs getAction, which calls the minimax function. This is a recursive function that
continues until it hits a base case: pacman wins, loses, or search hits the max depth allowed. This works
because once the base case is reached, the returned values from the base of the recursion tree travel back 
up to the root, alternating between being minimized and maximized by the ghosts and pacman until the 
optimal score and action returns to the root.


Q3.1:
The AlphaBetaAgent minimax values will be identical to the MinimaxAgent minimax values, because it only
skips branches that are guaranteed to not impact the final result. If a player finds a worse move than their
current option, then that player won't bother searching down that path since it won't be optimal anyways.


Q3.2
If multiple actions get the same optimal value, the program will go with the action that was visited first.
Alpha will only update if the new score is strictly greater than the current score, and the opposite is true for 
beta. The optimal action will only update if a following action has a greater score.


Q4.1:
The Expectimax algorithm is similar to minimax, except ghosts are treated as random instead of adversarial. 
On Pacman’s turn (MAX), we choose the action with the highest value. On ghost turns, instead of taking the 
minimum, we compute the expected value by averaging over all legal ghost moves (assuming each move is equally 
likely). The search stops at terminal states or when the depth limit is reached.

The behavior differs from AlphaBeta because minimax assumes ghosts always act in the worst possible way for Pacman. 
In trappedClassic, that worst-case assumption guarantees Pacman gets trapped, so AlphaBeta loses every time. 
Expectimax assumes ghosts move randomly, so Pacman is willing to take risky paths if the expected outcome is good. 
That’s why Expectimax wins some games while AlphaBeta always loses.

Q5.1:
My new evaluation function scores states using multiple factors instead of just reacting to the immediate move. 
It considers distance to food, remaining food count, ghost distance, scared timers, and capsules. It rewards being 
closer to food and finishing pellets, penalizes being near active ghosts, and encourages chasing ghosts when they’re 
scared. Capsules are valued more when ghosts are nearby.

This is better than my previous function because it evaluates the overall state strategically rather than short-term 
moves. The old one was more reactive, while this one balances survival, scoring, and progress toward winning, which 
is why it performs much better and consistently wins with high scores.

